{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APACHE FLINK PYTHON TABLE API\n",
    "\n",
    "This experience is a short introduction to the PyFlink Table API, which is used to help novice users quickly understand the basic usage of PyFlink Table API.\n",
    "\n",
    "Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. \n",
    "You can follow this <a href=\"https://flink.apache.org/\"> link </a> for more details.\n",
    "\n",
    "\n",
    "Apache Flink supports different runtime execution modes from which you can choose depending on the requirements of your use case and the characteristics of your job.\n",
    "\n",
    "One of these modes is the _streaming mode_, should be used for unbounded jobs that require continuous incremental processing and are expected to stay online indefinitely.\n",
    "\n",
    "The other one is _batch mode_, executes jobs in a way that is more reminiscent of batch processing frameworks such as MapReduce. This should be used for bounded jobs for which you have a known fixed input and which do not run continuously.\n",
    "\n",
    "<img src=\"img/flink_batch_streaming.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline Components:**\n",
    "\n",
    "* Apache Flink (Open Source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Pyflink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TableEnvironment is a central concept of the Table API and SQL integration.\n",
    "\n",
    "Create Table from element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyflink.table import TableEnvironment, EnvironmentSettings\n",
    "\n",
    "# create a batch TableEnvironment\n",
    "env_settings = EnvironmentSettings.in_batch_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "table = table_env.from_elements([(1,'Hi'),(2, 'Hello')])\n",
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = table_env.from_elements([(1,'Hi'),(2, 'Hello')], ['id','data'])\n",
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_without_schema = table_env.from_elements([(1,'Hi'),(2, 'Hello')], ['id','data'])\n",
    "\n",
    "#by default the type of the \"id\" column is 64 bit int\n",
    "\n",
    "default_type = table_without_schema.to_pandas()[\"id\"].dtype\n",
    "print('By default the type of the \"id\" column is %s.' % default_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manuel Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.table import DataTypes\n",
    "\n",
    "table=table_env.from_elements([(1,'Hi'),(2, 'Hello')],\n",
    "                             DataTypes.ROW([DataTypes.FIELD(\"id\", DataTypes.TINYINT()),\n",
    "                                           DataTypes.FIELD(\"data\", DataTypes.STRING())]))\n",
    "\n",
    "# now the type of the \"id\" column is 8 bit int\n",
    "\n",
    "type = table.to_pandas()[\"id\"].dtype\n",
    "print('Now the type of the \"id\" column is %s.' % type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using batch table environment to execute the queries\n",
    "\n",
    "env_settings = EnvironmentSettings.in_batch_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "orders = table_env.from_elements([('Jack', 'FRANCE', 10), ('Rose', 'ENGLAND', 30),('Jack', 'FRANCE', 20)], \n",
    "                                 ['name', 'country','revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute revenue for all customers from France\n",
    "\n",
    "revenue = orders \\\n",
    "        .select(orders.name, orders.country, orders.revenue) \\\n",
    "        .where(orders.country == 'FRANCE') \\\n",
    "        .group_by(orders.name) \\\n",
    "        .select(orders.name, orders.revenue.sum.alias('rev_sum'))\n",
    "\n",
    "revenue.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDF, User Define Function Row-Based Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyflink.table import TableEnvironment, EnvironmentSettings\n",
    "from pyflink.table import DataTypes\n",
    "from pyflink.table.udf import udf\n",
    "import pandas as pd\n",
    "\n",
    "# using batch table environment to execute the queries\n",
    "\n",
    "env_settings = EnvironmentSettings.in_batch_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "orders = table_env.from_elements([('Jack', 'FRANCE', 10), ('Rose', 'ENGLAND', 30),('Jack', 'FRANCE', 20)], \n",
    "                                 ['name', 'country','revenue'])\n",
    "\n",
    "#User define function\n",
    "map_function = udf(lambda x: pd.concat([x.name, x.revenue * 10], axis=1),\n",
    "                  result_type= DataTypes.ROW(\n",
    "                              [DataTypes.FIELD(\"name\", DataTypes.STRING()),\n",
    "                               DataTypes.FIELD(\"revenue\", DataTypes.BIGINT())]),\n",
    "                  func_type=\"pandas\")\n",
    "\n",
    "orders.map(map_function).alias('name','revenue').to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = table_env.from_elements([(1,\"Hi\", \"Hello\"),(2, \"Hello\", \"Hello\")], [\"a\",\"b\",\"c\"])\n",
    "\n",
    "# Get TableResult\n",
    "res = table_env.execute_sql(\"select a + 1, b, c from %s\" % source)\n",
    "\n",
    "#Travelsal result\n",
    "with res.collect() as results:\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_settings= EnvironmentSettings.in_streaming_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "#create a sql source table\n",
    "table_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE sql_source (\n",
    "        id BIGINT,\n",
    "        data TINYINT\n",
    "    ) WITH (\n",
    "        'connector' = 'datagen',\n",
    "        'fields.id.kind' = 'sequence',\n",
    "        'fields.id.start' = '1',\n",
    "        'fields.id.end' = '4',\n",
    "        'fields.data.kind' = 'sequence',\n",
    "        'fields.data.start' = '4',\n",
    "        'fields.data.end' = '7'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "#convert the sql table to Table API table\n",
    "table = table_env.from_path(\"sql_source\")\n",
    "\n",
    "#or create the table from a sql query\n",
    "table = table_env.sql_query(\" SELECT * FROM sql_source\")\n",
    "\n",
    "#emit the table\n",
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables and Explain and Lazy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyflink.table import TableEnvironment, EnvironmentSettings\n",
    "\n",
    "env_settings= EnvironmentSettings.in_streaming_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "table1 = table_env.from_elements([(1,'Hi'),(2, 'Hello')], ['id','data'])\n",
    "table2 = table_env.from_elements([(1,'Hi'),(2, 'Hello')], ['id','data'])\n",
    "\n",
    "table = table1 \\\n",
    "    .where(table1.data.like('H%')) \\\n",
    "    .union_all(table2)\n",
    "\n",
    "print(table.explain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get result\n",
    "table.to_pandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Pandas DataFrame to PyFlink Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "env_settings = EnvironmentSettings.in_batch_mode()\n",
    "t_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "#Create a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(1000,2))\n",
    "\n",
    "#Create a PyFlink Table from a Pandas DataFrame\n",
    "table = t_env.from_pandas(pdf)\n",
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a PyFlink Table from a Pandas DataFrame with the specified column names\n",
    "table = t_env.from_pandas(pdf, ['f0','f1'])\n",
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a PyFlink Table from a Pandas DataFrame with the specified column types\n",
    "table = t_env.from_pandas(pdf, [DataTypes.DOUBLE(), DataTypes.DOUBLE()])\n",
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create a PyFlink Table from a Pandas DataFrame with the specified row type\n",
    "table = t_env.from_pandas(pdf,\n",
    "                         DataTypes.ROW([DataTypes.FIELD(\"f0\",DataTypes.DOUBLE()),\n",
    "                                       DataTypes.FIELD(\"f1\", DataTypes.DOUBLE())]))\n",
    "\n",
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source and Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.table import TableEnvironment, EnvironmentSettings\n",
    "\n",
    "#use a stream TableEnvironment to execute the queries\n",
    "\n",
    "env_settings= EnvironmentSettings.in_streaming_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "table_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE random_source (\n",
    "        id BIGINT,\n",
    "        data TINYINT\n",
    "    ) WITH (\n",
    "        'connector' = 'datagen',\n",
    "        'fields.id.kind' = 'sequence',\n",
    "        'fields.id.start' = '1',\n",
    "        'fields.id.end' = '8',\n",
    "        'fields.data.kind' = 'sequence',\n",
    "        'fields.data.start' = '4',\n",
    "        'fields.data.end' = '11'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "table_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE print_sink (\n",
    "        id BIGINT,\n",
    "        data_sum TINYINT\n",
    "    ) WITH (\n",
    "        'connector' = 'print'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "table_env.execute_sql(\"\"\"\n",
    "    INSERT INTO print_sink\n",
    "        SELECT id,sum(data) as data_sum FROM\n",
    "            (SELECT id / 2 as id, data FROM random_source)\n",
    "        WHERE id > 1\n",
    "        GROUP BY id\n",
    "\"\"\").wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
